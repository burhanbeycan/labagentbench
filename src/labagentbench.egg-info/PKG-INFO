Metadata-Version: 2.4
Name: labagentbench
Version: 0.1.0
Summary: Benchmark suite for human- and literature-informed Bayesian optimisation.
Author: Burhan Beycan
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.24
Requires-Dist: pandas>=2.0
Requires-Dist: scipy>=1.10
Requires-Dist: scikit-learn>=1.3
Requires-Dist: typer>=0.9
Requires-Dist: rich>=13.0
Requires-Dist: matplotlib>=3.7
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Dynamic: license-file

# LabAgentBench — Benchmarks for literature- and human-informed BO

This mini-project provides a small benchmark suite to compare:

- **vanilla Bayesian optimisation**
- **text-prior BO** (hints → bounds/prior shaping)
- **preference-based optimisation** (simulated human comparisons)

The goal is a repo that can underpin a methods paper and show clean engineering practices.

---

## Run

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"

# run a text-hint task for 25 iterations
labagentbench run --task branin_text --iters 25 --seed 0
```

This writes results to `outputs/`.

---

## Extending

- Add your own tasks in `src/labagentbench/tasks.py`
- Add new “hint parsers” in `src/labagentbench/hints.py`
- Swap scikit-learn GP for BoTorch/qNEI when targeting publications
